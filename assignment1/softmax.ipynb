{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    \n",
    "    # Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "    try:\n",
    "       del X_train, y_train\n",
    "       del X_test, y_test\n",
    "       print('Clear previously loaded data.')\n",
    "    except:\n",
    "       pass\n",
    "\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs231n/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.378676\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "**Inline Question 1**\n",
    "\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "$\\color{blue}{\\textit Your Answer:}$ *Fill this in* \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -1.372334 analytic: -1.372334, relative error: 3.848943e-08\n",
      "numerical: 0.904032 analytic: 0.904032, relative error: 2.129730e-08\n",
      "numerical: 0.426976 analytic: 0.426976, relative error: 4.056916e-08\n",
      "numerical: 1.408007 analytic: 1.408007, relative error: 3.118424e-08\n",
      "numerical: 0.516204 analytic: 0.516204, relative error: 4.593056e-08\n",
      "numerical: -0.593501 analytic: -0.593502, relative error: 3.546929e-08\n",
      "numerical: -1.120498 analytic: -1.120498, relative error: 3.203523e-09\n",
      "numerical: -0.659762 analytic: -0.659762, relative error: 2.150643e-08\n",
      "numerical: -4.198776 analytic: -4.198776, relative error: 1.303274e-08\n",
      "numerical: -1.435355 analytic: -1.435355, relative error: 4.688649e-08\n",
      "numerical: -1.564959 analytic: -1.564959, relative error: 1.706623e-08\n",
      "numerical: 2.227693 analytic: 2.227693, relative error: 3.691690e-08\n",
      "numerical: -0.545691 analytic: -0.545691, relative error: 3.673879e-08\n",
      "numerical: -0.002106 analytic: -0.002106, relative error: 3.685319e-06\n",
      "numerical: -0.348951 analytic: -0.348951, relative error: 1.157188e-08\n",
      "numerical: 3.952653 analytic: 3.952652, relative error: 1.454437e-08\n",
      "numerical: -0.386502 analytic: -0.386502, relative error: 2.496151e-07\n",
      "numerical: 1.262059 analytic: 1.262058, relative error: 4.507003e-08\n",
      "numerical: 0.165124 analytic: 0.165124, relative error: 2.714586e-07\n",
      "numerical: -1.202877 analytic: -1.202877, relative error: 4.984174e-08\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.378676e+00 computed in 0.105717s\n",
      "vectorized loss: 2.378676e+00 computed in 0.001963s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [
     "code"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1500: loss 773.887424\n",
      "iteration 100 / 1500: loss 284.232568\n",
      "iteration 200 / 1500: loss 105.327820\n",
      "iteration 300 / 1500: loss 39.804563\n",
      "iteration 400 / 1500: loss 15.888788\n",
      "iteration 500 / 1500: loss 7.132969\n",
      "iteration 600 / 1500: loss 3.941938\n",
      "iteration 700 / 1500: loss 2.811446\n",
      "iteration 800 / 1500: loss 2.342113\n",
      "iteration 900 / 1500: loss 2.130057\n",
      "iteration 1000 / 1500: loss 2.127157\n",
      "iteration 1100 / 1500: loss 2.091804\n",
      "iteration 1200 / 1500: loss 2.067490\n",
      "iteration 1300 / 1500: loss 2.026890\n",
      "iteration 1400 / 1500: loss 2.067093\n",
      "iteration 0 / 1500: loss 1563.622803\n",
      "iteration 100 / 1500: loss 210.624209\n",
      "iteration 200 / 1500: loss 30.039139\n",
      "iteration 300 / 1500: loss 5.873536\n",
      "iteration 400 / 1500: loss 2.621759\n",
      "iteration 500 / 1500: loss 2.231255\n",
      "iteration 600 / 1500: loss 2.184452\n",
      "iteration 700 / 1500: loss 2.175142\n",
      "iteration 800 / 1500: loss 2.143386\n",
      "iteration 900 / 1500: loss 2.116088\n",
      "iteration 1000 / 1500: loss 2.169799\n",
      "iteration 1100 / 1500: loss 2.096732\n",
      "iteration 1200 / 1500: loss 2.195520\n",
      "iteration 1300 / 1500: loss 2.141207\n",
      "iteration 1400 / 1500: loss 2.114790\n",
      "iteration 0 / 1500: loss 787.160894\n",
      "iteration 100 / 1500: loss 7.054350\n",
      "iteration 200 / 1500: loss 2.109379\n",
      "iteration 300 / 1500: loss 2.048847\n",
      "iteration 400 / 1500: loss 2.115861\n",
      "iteration 500 / 1500: loss 2.097025\n",
      "iteration 600 / 1500: loss 2.096117\n",
      "iteration 700 / 1500: loss 2.090684\n",
      "iteration 800 / 1500: loss 2.077900\n",
      "iteration 900 / 1500: loss 2.098694\n",
      "iteration 1000 / 1500: loss 2.091640\n",
      "iteration 1100 / 1500: loss 2.111540\n",
      "iteration 1200 / 1500: loss 2.066597\n",
      "iteration 1300 / 1500: loss 2.115543\n",
      "iteration 1400 / 1500: loss 2.121364\n",
      "iteration 0 / 1500: loss 1533.023503\n",
      "iteration 100 / 1500: loss 2.181436\n",
      "iteration 200 / 1500: loss 2.161924\n",
      "iteration 300 / 1500: loss 2.183153\n",
      "iteration 400 / 1500: loss 2.107810\n",
      "iteration 500 / 1500: loss 2.124384\n",
      "iteration 600 / 1500: loss 2.151661\n",
      "iteration 700 / 1500: loss 2.122080\n",
      "iteration 800 / 1500: loss 2.127770\n",
      "iteration 900 / 1500: loss 2.157221\n",
      "iteration 1000 / 1500: loss 2.151046\n",
      "iteration 1100 / 1500: loss 2.168710\n",
      "iteration 1200 / 1500: loss 2.128329\n",
      "iteration 1300 / 1500: loss 2.131376\n",
      "iteration 1400 / 1500: loss 2.087703\n",
      "iteration 0 / 1500: loss 770.919091\n",
      "iteration 100 / 1500: loss 2.204694\n",
      "iteration 200 / 1500: loss 2.105616\n",
      "iteration 300 / 1500: loss 2.062263\n",
      "iteration 400 / 1500: loss 2.169449\n",
      "iteration 500 / 1500: loss 2.110276\n",
      "iteration 600 / 1500: loss 2.083043\n",
      "iteration 700 / 1500: loss 2.078439\n",
      "iteration 800 / 1500: loss 2.091171\n",
      "iteration 900 / 1500: loss 2.082550\n",
      "iteration 1000 / 1500: loss 2.174046\n",
      "iteration 1100 / 1500: loss 2.042492\n",
      "iteration 1200 / 1500: loss 2.088135\n",
      "iteration 1300 / 1500: loss 2.118324\n",
      "iteration 1400 / 1500: loss 2.061054\n",
      "iteration 0 / 1500: loss 1544.735246\n",
      "iteration 100 / 1500: loss 2.137894\n",
      "iteration 200 / 1500: loss 2.149181\n",
      "iteration 300 / 1500: loss 2.138324\n",
      "iteration 400 / 1500: loss 2.153848\n",
      "iteration 500 / 1500: loss 2.141621\n",
      "iteration 600 / 1500: loss 2.169329\n",
      "iteration 700 / 1500: loss 2.109994\n",
      "iteration 800 / 1500: loss 2.183434\n",
      "iteration 900 / 1500: loss 2.161772\n",
      "iteration 1000 / 1500: loss 2.118548\n",
      "iteration 1100 / 1500: loss 2.156790\n",
      "iteration 1200 / 1500: loss 2.194182\n",
      "iteration 1300 / 1500: loss 2.078656\n",
      "iteration 1400 / 1500: loss 2.164385\n",
      "iteration 0 / 1500: loss 772.559124\n",
      "iteration 100 / 1500: loss 2.092337\n",
      "iteration 200 / 1500: loss 2.087514\n",
      "iteration 300 / 1500: loss 2.128908\n",
      "iteration 400 / 1500: loss 2.108692\n",
      "iteration 500 / 1500: loss 2.207103\n",
      "iteration 600 / 1500: loss 2.120642\n",
      "iteration 700 / 1500: loss 2.117493\n",
      "iteration 800 / 1500: loss 2.123315\n",
      "iteration 900 / 1500: loss 2.055214\n",
      "iteration 1000 / 1500: loss 2.101681\n",
      "iteration 1100 / 1500: loss 2.130972\n",
      "iteration 1200 / 1500: loss 2.111847\n",
      "iteration 1300 / 1500: loss 2.127771\n",
      "iteration 1400 / 1500: loss 2.113478\n",
      "iteration 0 / 1500: loss 1530.022779\n",
      "iteration 100 / 1500: loss 2.152501\n",
      "iteration 200 / 1500: loss 2.174859\n",
      "iteration 300 / 1500: loss 2.185158\n",
      "iteration 400 / 1500: loss 2.215536\n",
      "iteration 500 / 1500: loss 2.196260\n",
      "iteration 600 / 1500: loss 2.122053\n",
      "iteration 700 / 1500: loss 2.129697\n",
      "iteration 800 / 1500: loss 2.154125\n",
      "iteration 900 / 1500: loss 2.163775\n",
      "iteration 1000 / 1500: loss 2.196886\n",
      "iteration 1100 / 1500: loss 2.136149\n",
      "iteration 1200 / 1500: loss 2.231962\n",
      "iteration 1300 / 1500: loss 2.102968\n",
      "iteration 1400 / 1500: loss 2.175304\n",
      "iteration 0 / 1500: loss 776.095064\n",
      "iteration 100 / 1500: loss 2.107923\n",
      "iteration 200 / 1500: loss 2.051091\n",
      "iteration 300 / 1500: loss 2.086457\n",
      "iteration 400 / 1500: loss 2.127585\n",
      "iteration 500 / 1500: loss 2.026911\n",
      "iteration 600 / 1500: loss 2.080381\n",
      "iteration 700 / 1500: loss 2.078861\n",
      "iteration 800 / 1500: loss 2.146380\n",
      "iteration 900 / 1500: loss 2.078706\n",
      "iteration 1000 / 1500: loss 2.116107\n",
      "iteration 1100 / 1500: loss 2.135589\n",
      "iteration 1200 / 1500: loss 2.064701\n",
      "iteration 1300 / 1500: loss 2.121190\n",
      "iteration 1400 / 1500: loss 2.116069\n",
      "iteration 0 / 1500: loss 1528.658153\n",
      "iteration 100 / 1500: loss 2.206267\n",
      "iteration 200 / 1500: loss 2.178145\n",
      "iteration 300 / 1500: loss 2.143590\n",
      "iteration 400 / 1500: loss 2.159193\n",
      "iteration 500 / 1500: loss 2.235755\n",
      "iteration 600 / 1500: loss 2.180713\n",
      "iteration 700 / 1500: loss 2.170465\n",
      "iteration 800 / 1500: loss 2.157880\n",
      "iteration 900 / 1500: loss 2.135258\n",
      "iteration 1000 / 1500: loss 2.142135\n",
      "iteration 1100 / 1500: loss 2.196522\n",
      "iteration 1200 / 1500: loss 2.138803\n",
      "iteration 1300 / 1500: loss 2.216091\n",
      "iteration 1400 / 1500: loss 2.158188\n",
      "iteration 0 / 1500: loss 761.962613\n",
      "iteration 100 / 1500: loss 2.164877\n",
      "iteration 200 / 1500: loss 2.100200\n",
      "iteration 300 / 1500: loss 2.215977\n",
      "iteration 400 / 1500: loss 2.090017\n",
      "iteration 500 / 1500: loss 2.119332\n",
      "iteration 600 / 1500: loss 2.128381\n",
      "iteration 700 / 1500: loss 2.221417\n",
      "iteration 800 / 1500: loss 2.167180\n",
      "iteration 900 / 1500: loss 2.151941\n",
      "iteration 1000 / 1500: loss 2.110545\n",
      "iteration 1100 / 1500: loss 2.147722\n",
      "iteration 1200 / 1500: loss 2.118538\n",
      "iteration 1300 / 1500: loss 2.062411\n",
      "iteration 1400 / 1500: loss 2.131492\n",
      "iteration 0 / 1500: loss 1531.301722\n",
      "iteration 100 / 1500: loss 2.113720\n",
      "iteration 200 / 1500: loss 2.200845\n",
      "iteration 300 / 1500: loss 2.186657\n",
      "iteration 400 / 1500: loss 2.161912\n",
      "iteration 500 / 1500: loss 2.107589\n",
      "iteration 600 / 1500: loss 2.170456\n",
      "iteration 700 / 1500: loss 2.157467\n",
      "iteration 800 / 1500: loss 2.173204\n",
      "iteration 900 / 1500: loss 2.182669\n",
      "iteration 1000 / 1500: loss 2.147658\n",
      "iteration 1100 / 1500: loss 2.143746\n",
      "iteration 1200 / 1500: loss 2.227435\n",
      "iteration 1300 / 1500: loss 2.188854\n",
      "iteration 1400 / 1500: loss 2.181881\n",
      "iteration 0 / 1500: loss 773.728594\n",
      "iteration 100 / 1500: loss 2.123667\n",
      "iteration 200 / 1500: loss 2.147412\n",
      "iteration 300 / 1500: loss 2.185099\n",
      "iteration 400 / 1500: loss 2.111111\n",
      "iteration 500 / 1500: loss 2.135400\n",
      "iteration 600 / 1500: loss 2.109748\n",
      "iteration 700 / 1500: loss 2.148580\n",
      "iteration 800 / 1500: loss 2.166376\n",
      "iteration 900 / 1500: loss 2.123592\n",
      "iteration 1000 / 1500: loss 2.130566\n",
      "iteration 1100 / 1500: loss 2.119766\n",
      "iteration 1200 / 1500: loss 2.136135\n",
      "iteration 1300 / 1500: loss 2.103701\n",
      "iteration 1400 / 1500: loss 2.176247\n",
      "iteration 0 / 1500: loss 1530.328333\n",
      "iteration 100 / 1500: loss 2.226959\n",
      "iteration 200 / 1500: loss 2.224689\n",
      "iteration 300 / 1500: loss 2.216039\n",
      "iteration 400 / 1500: loss 2.205954\n",
      "iteration 500 / 1500: loss 2.242257\n",
      "iteration 600 / 1500: loss 2.170954\n",
      "iteration 700 / 1500: loss 2.219380\n",
      "iteration 800 / 1500: loss 2.284137\n",
      "iteration 900 / 1500: loss 2.230046\n",
      "iteration 1000 / 1500: loss 2.209754\n",
      "iteration 1100 / 1500: loss 2.180701\n",
      "iteration 1200 / 1500: loss 2.214277\n",
      "iteration 1300 / 1500: loss 2.210222\n",
      "iteration 1400 / 1500: loss 2.125730\n",
      "iteration 0 / 1500: loss 768.796286\n",
      "iteration 100 / 1500: loss 2.171179\n",
      "iteration 200 / 1500: loss 2.158127\n",
      "iteration 300 / 1500: loss 2.041744\n",
      "iteration 400 / 1500: loss 2.167003\n",
      "iteration 500 / 1500: loss 2.148764\n",
      "iteration 600 / 1500: loss 2.148273\n",
      "iteration 700 / 1500: loss 2.179261\n",
      "iteration 800 / 1500: loss 2.095827\n",
      "iteration 900 / 1500: loss 2.136664\n",
      "iteration 1000 / 1500: loss 2.121452\n",
      "iteration 1100 / 1500: loss 2.109123\n",
      "iteration 1200 / 1500: loss 2.125601\n",
      "iteration 1300 / 1500: loss 2.143551\n",
      "iteration 1400 / 1500: loss 2.139428\n",
      "iteration 0 / 1500: loss 1545.210519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 1500: loss 2.258104\n",
      "iteration 200 / 1500: loss 2.235928\n",
      "iteration 300 / 1500: loss 2.216015\n",
      "iteration 400 / 1500: loss 2.218021\n",
      "iteration 500 / 1500: loss 2.216128\n",
      "iteration 600 / 1500: loss 2.203913\n",
      "iteration 700 / 1500: loss 2.195359\n",
      "iteration 800 / 1500: loss 2.223137\n",
      "iteration 900 / 1500: loss 2.202602\n",
      "iteration 1000 / 1500: loss 2.218403\n",
      "iteration 1100 / 1500: loss 2.241150\n",
      "iteration 1200 / 1500: loss 2.211702\n",
      "iteration 1300 / 1500: loss 2.172810\n",
      "iteration 1400 / 1500: loss 2.150569\n",
      "iteration 0 / 1500: loss 761.039392\n",
      "iteration 100 / 1500: loss 2.125433\n",
      "iteration 200 / 1500: loss 2.200962\n",
      "iteration 300 / 1500: loss 2.155036\n",
      "iteration 400 / 1500: loss 2.128371\n",
      "iteration 500 / 1500: loss 2.125478\n",
      "iteration 600 / 1500: loss 2.084886\n",
      "iteration 700 / 1500: loss 2.153150\n",
      "iteration 800 / 1500: loss 2.138556\n",
      "iteration 900 / 1500: loss 2.190443\n",
      "iteration 1000 / 1500: loss 2.203520\n",
      "iteration 1100 / 1500: loss 2.170243\n",
      "iteration 1200 / 1500: loss 2.111121\n",
      "iteration 1300 / 1500: loss 2.120153\n",
      "iteration 1400 / 1500: loss 2.187306\n",
      "iteration 0 / 1500: loss 1528.409344\n",
      "iteration 100 / 1500: loss 2.347298\n",
      "iteration 200 / 1500: loss 2.255718\n",
      "iteration 300 / 1500: loss 2.300792\n",
      "iteration 400 / 1500: loss 2.222467\n",
      "iteration 500 / 1500: loss 2.187572\n",
      "iteration 600 / 1500: loss 2.275466\n",
      "iteration 700 / 1500: loss 2.194358\n",
      "iteration 800 / 1500: loss 2.247605\n",
      "iteration 900 / 1500: loss 2.224516\n",
      "iteration 1000 / 1500: loss 2.233186\n",
      "iteration 1100 / 1500: loss 2.168718\n",
      "iteration 1200 / 1500: loss 2.258097\n",
      "iteration 1300 / 1500: loss 2.317822\n",
      "iteration 1400 / 1500: loss 2.254214\n",
      "iteration 0 / 1500: loss 771.906197\n",
      "iteration 100 / 1500: loss 2.189534\n",
      "iteration 200 / 1500: loss 2.228206\n",
      "iteration 300 / 1500: loss 2.156861\n",
      "iteration 400 / 1500: loss 2.202877\n",
      "iteration 500 / 1500: loss 2.241969\n",
      "iteration 600 / 1500: loss 2.148216\n",
      "iteration 700 / 1500: loss 2.236652\n",
      "iteration 800 / 1500: loss 2.126680\n",
      "iteration 900 / 1500: loss 2.318438\n",
      "iteration 1000 / 1500: loss 2.071753\n",
      "iteration 1100 / 1500: loss 2.262206\n",
      "iteration 1200 / 1500: loss 2.201579\n",
      "iteration 1300 / 1500: loss 2.153218\n",
      "iteration 1400 / 1500: loss 2.148473\n",
      "iteration 0 / 1500: loss 1520.010561\n",
      "iteration 100 / 1500: loss 2.437020\n",
      "iteration 200 / 1500: loss 2.278384\n",
      "iteration 300 / 1500: loss 2.223840\n",
      "iteration 400 / 1500: loss 2.216401\n",
      "iteration 500 / 1500: loss 2.363807\n",
      "iteration 600 / 1500: loss 2.327235\n",
      "iteration 700 / 1500: loss 2.168446\n",
      "iteration 800 / 1500: loss 2.178981\n",
      "iteration 900 / 1500: loss 2.214007\n",
      "iteration 1000 / 1500: loss 2.288133\n",
      "iteration 1100 / 1500: loss 2.231827\n",
      "iteration 1200 / 1500: loss 2.221832\n",
      "iteration 1300 / 1500: loss 2.296286\n",
      "iteration 1400 / 1500: loss 2.324561\n",
      "iteration 0 / 1500: loss 780.546336\n",
      "iteration 100 / 1500: loss 2.139410\n",
      "iteration 200 / 1500: loss 2.150000\n",
      "iteration 300 / 1500: loss 2.062164\n",
      "iteration 400 / 1500: loss 2.185955\n",
      "iteration 500 / 1500: loss 2.464678\n",
      "iteration 600 / 1500: loss 2.161222\n",
      "iteration 700 / 1500: loss 2.235652\n",
      "iteration 800 / 1500: loss 2.212283\n",
      "iteration 900 / 1500: loss 2.169872\n",
      "iteration 1000 / 1500: loss 2.239105\n",
      "iteration 1100 / 1500: loss 2.272470\n",
      "iteration 1200 / 1500: loss 2.160687\n",
      "iteration 1300 / 1500: loss 2.169607\n",
      "iteration 1400 / 1500: loss 2.123828\n",
      "iteration 0 / 1500: loss 1555.920539\n",
      "iteration 100 / 1500: loss 2.804773\n",
      "iteration 200 / 1500: loss 2.339477\n",
      "iteration 300 / 1500: loss 2.771552\n",
      "iteration 400 / 1500: loss 2.247765\n",
      "iteration 500 / 1500: loss 2.341185\n",
      "iteration 600 / 1500: loss 2.242045\n",
      "iteration 700 / 1500: loss 2.253515\n",
      "iteration 800 / 1500: loss 2.286412\n",
      "iteration 900 / 1500: loss 2.443720\n",
      "iteration 1000 / 1500: loss 2.402289\n",
      "iteration 1100 / 1500: loss 2.702317\n",
      "iteration 1200 / 1500: loss 2.309404\n",
      "iteration 1300 / 1500: loss 2.760295\n",
      "iteration 1400 / 1500: loss 2.423170\n",
      "iteration 0 / 1500: loss 772.080197\n",
      "iteration 100 / 1500: loss 2.307157\n",
      "iteration 200 / 1500: loss 2.179627\n",
      "iteration 300 / 1500: loss 2.337249\n",
      "iteration 400 / 1500: loss 2.239756\n",
      "iteration 500 / 1500: loss 2.270544\n",
      "iteration 600 / 1500: loss 2.225817\n",
      "iteration 700 / 1500: loss 2.275857\n",
      "iteration 800 / 1500: loss 2.370987\n",
      "iteration 900 / 1500: loss 2.321067\n",
      "iteration 1000 / 1500: loss 2.426937\n",
      "iteration 1100 / 1500: loss 2.308643\n",
      "iteration 1200 / 1500: loss 2.256297\n",
      "iteration 1300 / 1500: loss 2.141149\n",
      "iteration 1400 / 1500: loss 2.222113\n",
      "iteration 0 / 1500: loss 1544.411926\n",
      "iteration 100 / 1500: loss 2.654636\n",
      "iteration 200 / 1500: loss 3.088149\n",
      "iteration 300 / 1500: loss 2.320680\n",
      "iteration 400 / 1500: loss 3.755312\n",
      "iteration 500 / 1500: loss 2.657773\n",
      "iteration 600 / 1500: loss 2.903587\n",
      "iteration 700 / 1500: loss 2.989730\n",
      "iteration 800 / 1500: loss 3.287311\n",
      "iteration 900 / 1500: loss 3.055144\n",
      "iteration 1000 / 1500: loss 2.951126\n",
      "iteration 1100 / 1500: loss 3.021125\n",
      "iteration 1200 / 1500: loss 2.416608\n",
      "iteration 1300 / 1500: loss 3.100599\n",
      "iteration 1400 / 1500: loss 2.669028\n",
      "iteration 0 / 1500: loss 771.286829\n",
      "iteration 100 / 1500: loss 2.199732\n",
      "iteration 200 / 1500: loss 2.347433\n",
      "iteration 300 / 1500: loss 2.634684\n",
      "iteration 400 / 1500: loss 2.527361\n",
      "iteration 500 / 1500: loss 2.308171\n",
      "iteration 600 / 1500: loss 2.496895\n",
      "iteration 700 / 1500: loss 2.487913\n",
      "iteration 800 / 1500: loss 2.543725\n",
      "iteration 900 / 1500: loss 2.896416\n",
      "iteration 1000 / 1500: loss 2.257223\n",
      "iteration 1100 / 1500: loss 2.200058\n",
      "iteration 1200 / 1500: loss 2.372262\n",
      "iteration 1300 / 1500: loss 2.285911\n",
      "iteration 1400 / 1500: loss 2.257105\n",
      "iteration 0 / 1500: loss 1531.521208\n",
      "iteration 100 / 1500: loss 4.137516\n",
      "iteration 200 / 1500: loss 4.274007\n",
      "iteration 300 / 1500: loss 2.937263\n",
      "iteration 400 / 1500: loss 3.040300\n",
      "iteration 500 / 1500: loss 2.840191\n",
      "iteration 600 / 1500: loss 2.889579\n",
      "iteration 700 / 1500: loss 3.190635\n",
      "iteration 800 / 1500: loss 3.300581\n",
      "iteration 900 / 1500: loss 3.444580\n",
      "iteration 1000 / 1500: loss 3.615286\n",
      "iteration 1100 / 1500: loss 3.494002\n",
      "iteration 1200 / 1500: loss 2.928001\n",
      "iteration 1300 / 1500: loss 3.640932\n",
      "iteration 1400 / 1500: loss 3.419276\n",
      "iteration 0 / 1500: loss 778.226934\n",
      "iteration 100 / 1500: loss 2.770665\n",
      "iteration 200 / 1500: loss 2.580050\n",
      "iteration 300 / 1500: loss 2.514584\n",
      "iteration 400 / 1500: loss 3.311483\n",
      "iteration 500 / 1500: loss 2.844615\n",
      "iteration 600 / 1500: loss 3.309141\n",
      "iteration 700 / 1500: loss 3.525922\n",
      "iteration 800 / 1500: loss 3.059180\n",
      "iteration 900 / 1500: loss 3.290527\n",
      "iteration 1000 / 1500: loss 2.823376\n",
      "iteration 1100 / 1500: loss 2.814806\n",
      "iteration 1200 / 1500: loss 2.798645\n",
      "iteration 1300 / 1500: loss 2.540135\n",
      "iteration 1400 / 1500: loss 2.539634\n",
      "iteration 0 / 1500: loss 1544.374415\n",
      "iteration 100 / 1500: loss 3.518945\n",
      "iteration 200 / 1500: loss 4.807954\n",
      "iteration 300 / 1500: loss 5.459184\n",
      "iteration 400 / 1500: loss 4.291163\n",
      "iteration 500 / 1500: loss 3.442669\n",
      "iteration 600 / 1500: loss 3.955673\n",
      "iteration 700 / 1500: loss 4.767993\n",
      "iteration 800 / 1500: loss 3.975614\n",
      "iteration 900 / 1500: loss 4.018834\n",
      "iteration 1000 / 1500: loss 3.254082\n",
      "iteration 1100 / 1500: loss 5.102306\n",
      "iteration 1200 / 1500: loss 3.911286\n",
      "iteration 1300 / 1500: loss 3.796101\n",
      "iteration 1400 / 1500: loss 4.064365\n",
      "iteration 0 / 1500: loss 775.713591\n",
      "iteration 100 / 1500: loss 3.085601\n",
      "iteration 200 / 1500: loss 3.330526\n",
      "iteration 300 / 1500: loss 2.683644\n",
      "iteration 400 / 1500: loss 3.212093\n",
      "iteration 500 / 1500: loss 4.073815\n",
      "iteration 600 / 1500: loss 3.659103\n",
      "iteration 700 / 1500: loss 3.049507\n",
      "iteration 800 / 1500: loss 3.025991\n",
      "iteration 900 / 1500: loss 2.645682\n",
      "iteration 1000 / 1500: loss 3.352277\n",
      "iteration 1100 / 1500: loss 3.112771\n",
      "iteration 1200 / 1500: loss 3.136270\n",
      "iteration 1300 / 1500: loss 3.297347\n",
      "iteration 1400 / 1500: loss 3.365661\n",
      "iteration 0 / 1500: loss 1559.930503\n",
      "iteration 100 / 1500: loss 4.520541\n",
      "iteration 200 / 1500: loss 4.697817\n",
      "iteration 300 / 1500: loss 5.931534\n",
      "iteration 400 / 1500: loss 3.218630\n",
      "iteration 500 / 1500: loss 4.649613\n",
      "iteration 600 / 1500: loss 4.285050\n",
      "iteration 700 / 1500: loss 5.155080\n",
      "iteration 800 / 1500: loss 4.445096\n",
      "iteration 900 / 1500: loss 3.643084\n",
      "iteration 1000 / 1500: loss 4.435497\n",
      "iteration 1100 / 1500: loss 5.562475\n",
      "iteration 1200 / 1500: loss 3.571386\n",
      "iteration 1300 / 1500: loss 4.368296\n",
      "iteration 1400 / 1500: loss 4.750263\n",
      "iteration 0 / 1500: loss 764.626099\n",
      "iteration 100 / 1500: loss 4.382307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 200 / 1500: loss 3.272464\n",
      "iteration 300 / 1500: loss 2.736291\n",
      "iteration 400 / 1500: loss 3.807776\n",
      "iteration 500 / 1500: loss 4.924820\n",
      "iteration 600 / 1500: loss 2.945531\n",
      "iteration 700 / 1500: loss 3.628338\n",
      "iteration 800 / 1500: loss 2.749809\n",
      "iteration 900 / 1500: loss 2.958591\n",
      "iteration 1000 / 1500: loss 3.167532\n",
      "iteration 1100 / 1500: loss 3.258147\n",
      "iteration 1200 / 1500: loss 3.455854\n",
      "iteration 1300 / 1500: loss 3.066103\n",
      "iteration 1400 / 1500: loss 3.263344\n",
      "iteration 0 / 1500: loss 1564.445794\n",
      "iteration 100 / 1500: loss 5.122977\n",
      "iteration 200 / 1500: loss 6.369879\n",
      "iteration 300 / 1500: loss 6.122887\n",
      "iteration 400 / 1500: loss 5.416167\n",
      "iteration 500 / 1500: loss 4.427662\n",
      "iteration 600 / 1500: loss 6.183740\n",
      "iteration 700 / 1500: loss 7.327406\n",
      "iteration 800 / 1500: loss 5.452758\n",
      "iteration 900 / 1500: loss 4.908510\n",
      "iteration 1000 / 1500: loss 5.740634\n",
      "iteration 1100 / 1500: loss 5.433063\n",
      "iteration 1200 / 1500: loss 6.657967\n",
      "iteration 1300 / 1500: loss 5.565836\n",
      "iteration 1400 / 1500: loss 4.580366\n",
      "iteration 0 / 1500: loss 775.620499\n",
      "iteration 100 / 1500: loss 3.104655\n",
      "iteration 200 / 1500: loss 2.919012\n",
      "iteration 300 / 1500: loss 3.046426\n",
      "iteration 400 / 1500: loss 2.871987\n",
      "iteration 500 / 1500: loss 4.074861\n",
      "iteration 600 / 1500: loss 3.963055\n",
      "iteration 700 / 1500: loss 4.210857\n",
      "iteration 800 / 1500: loss 4.570396\n",
      "iteration 900 / 1500: loss 4.107456\n",
      "iteration 1000 / 1500: loss 4.067253\n",
      "iteration 1100 / 1500: loss 4.554877\n",
      "iteration 1200 / 1500: loss 3.278990\n",
      "iteration 1300 / 1500: loss 3.899114\n",
      "iteration 1400 / 1500: loss 3.613667\n",
      "iteration 0 / 1500: loss 1539.031904\n",
      "iteration 100 / 1500: loss 5.076909\n",
      "iteration 200 / 1500: loss 5.461756\n",
      "iteration 300 / 1500: loss 5.313192\n",
      "iteration 400 / 1500: loss 6.612408\n",
      "iteration 500 / 1500: loss 7.210556\n",
      "iteration 600 / 1500: loss 6.261515\n",
      "iteration 700 / 1500: loss 4.953413\n",
      "iteration 800 / 1500: loss 5.199052\n",
      "iteration 900 / 1500: loss 4.778878\n",
      "iteration 1000 / 1500: loss 6.806239\n",
      "iteration 1100 / 1500: loss 5.270141\n",
      "iteration 1200 / 1500: loss 5.488389\n",
      "iteration 1300 / 1500: loss 5.242314\n",
      "iteration 1400 / 1500: loss 6.217095\n",
      "iteration 0 / 1500: loss 776.558685\n",
      "iteration 100 / 1500: loss 4.798698\n",
      "iteration 200 / 1500: loss 3.792576\n",
      "iteration 300 / 1500: loss 4.186437\n",
      "iteration 400 / 1500: loss 3.320286\n",
      "iteration 500 / 1500: loss 4.655256\n",
      "iteration 600 / 1500: loss 4.104888\n",
      "iteration 700 / 1500: loss 4.347483\n",
      "iteration 800 / 1500: loss 4.262923\n",
      "iteration 900 / 1500: loss 3.421517\n",
      "iteration 1000 / 1500: loss 3.996883\n",
      "iteration 1100 / 1500: loss 4.146672\n",
      "iteration 1200 / 1500: loss 4.120271\n",
      "iteration 1300 / 1500: loss 4.144310\n",
      "iteration 1400 / 1500: loss 4.895186\n",
      "iteration 0 / 1500: loss 1550.495718\n",
      "iteration 100 / 1500: loss 5.398089\n",
      "iteration 200 / 1500: loss 6.332865\n",
      "iteration 300 / 1500: loss 6.342151\n",
      "iteration 400 / 1500: loss 5.541875\n",
      "iteration 500 / 1500: loss 5.770966\n",
      "iteration 600 / 1500: loss 6.346761\n",
      "iteration 700 / 1500: loss 6.349318\n",
      "iteration 800 / 1500: loss 6.379053\n",
      "iteration 900 / 1500: loss 7.168840\n",
      "iteration 1000 / 1500: loss 6.733596\n",
      "iteration 1100 / 1500: loss 6.511855\n",
      "iteration 1200 / 1500: loss 5.705024\n",
      "iteration 1300 / 1500: loss 5.284211\n",
      "iteration 1400 / 1500: loss 6.343153\n",
      "iteration 0 / 1500: loss 778.205709\n",
      "iteration 100 / 1500: loss 3.930751\n",
      "iteration 200 / 1500: loss 4.130354\n",
      "iteration 300 / 1500: loss 5.095187\n",
      "iteration 400 / 1500: loss 5.929403\n",
      "iteration 500 / 1500: loss 4.282486\n",
      "iteration 600 / 1500: loss 4.548278\n",
      "iteration 700 / 1500: loss 4.163917\n",
      "iteration 800 / 1500: loss 3.568261\n",
      "iteration 900 / 1500: loss 4.662196\n",
      "iteration 1000 / 1500: loss 5.552623\n",
      "iteration 1100 / 1500: loss 5.123800\n",
      "iteration 1200 / 1500: loss 5.634531\n",
      "iteration 1300 / 1500: loss 3.790244\n",
      "iteration 1400 / 1500: loss 5.107942\n",
      "iteration 0 / 1500: loss 1538.965452\n",
      "iteration 100 / 1500: loss 8.436027\n",
      "iteration 200 / 1500: loss 8.091973\n",
      "iteration 300 / 1500: loss 8.124108\n",
      "iteration 400 / 1500: loss 9.155718\n",
      "iteration 500 / 1500: loss 6.908690\n",
      "iteration 600 / 1500: loss 8.643008\n",
      "iteration 700 / 1500: loss 6.443503\n",
      "iteration 800 / 1500: loss 8.132213\n",
      "iteration 900 / 1500: loss 7.644244\n",
      "iteration 1000 / 1500: loss 7.803002\n",
      "iteration 1100 / 1500: loss 7.999860\n",
      "iteration 1200 / 1500: loss 7.174571\n",
      "iteration 1300 / 1500: loss 7.177261\n",
      "iteration 1400 / 1500: loss 7.408849\n",
      "iteration 0 / 1500: loss 766.096745\n",
      "iteration 100 / 1500: loss 4.592667\n",
      "iteration 200 / 1500: loss 4.781593\n",
      "iteration 300 / 1500: loss 4.261838\n",
      "iteration 400 / 1500: loss 5.752830\n",
      "iteration 500 / 1500: loss 6.498568\n",
      "iteration 600 / 1500: loss 5.027072\n",
      "iteration 700 / 1500: loss 6.518987\n",
      "iteration 800 / 1500: loss 5.347494\n",
      "iteration 900 / 1500: loss 5.585848\n",
      "iteration 1000 / 1500: loss 4.157461\n",
      "iteration 1100 / 1500: loss 5.439249\n",
      "iteration 1200 / 1500: loss 4.967702\n",
      "iteration 1300 / 1500: loss 3.596473\n",
      "iteration 1400 / 1500: loss 3.820550\n",
      "iteration 0 / 1500: loss 1549.713446\n",
      "iteration 100 / 1500: loss 6.851781\n",
      "iteration 200 / 1500: loss 9.015668\n",
      "iteration 300 / 1500: loss 7.730588\n",
      "iteration 400 / 1500: loss 8.152846\n",
      "iteration 500 / 1500: loss 8.457905\n",
      "iteration 600 / 1500: loss 9.068148\n",
      "iteration 700 / 1500: loss 9.143512\n",
      "iteration 800 / 1500: loss 7.797766\n",
      "iteration 900 / 1500: loss 10.517005\n",
      "iteration 1000 / 1500: loss 7.924197\n",
      "iteration 1100 / 1500: loss 6.219350\n",
      "iteration 1200 / 1500: loss 7.944190\n",
      "iteration 1300 / 1500: loss 7.923952\n",
      "iteration 1400 / 1500: loss 9.543295\n",
      "iteration 0 / 1500: loss 773.492021\n",
      "iteration 100 / 1500: loss 5.762468\n",
      "iteration 200 / 1500: loss 6.728988\n",
      "iteration 300 / 1500: loss 3.560003\n",
      "iteration 400 / 1500: loss 5.202604\n",
      "iteration 500 / 1500: loss 5.227273\n",
      "iteration 600 / 1500: loss 5.009219\n",
      "iteration 700 / 1500: loss 5.670608\n",
      "iteration 800 / 1500: loss 4.835810\n",
      "iteration 900 / 1500: loss 5.823306\n",
      "iteration 1000 / 1500: loss 4.920196\n",
      "iteration 1100 / 1500: loss 6.193340\n",
      "iteration 1200 / 1500: loss 4.180032\n",
      "iteration 1300 / 1500: loss 4.204427\n",
      "iteration 1400 / 1500: loss 3.693527\n",
      "iteration 0 / 1500: loss 1550.359350\n",
      "iteration 100 / 1500: loss 10.963873\n",
      "iteration 200 / 1500: loss 8.820861\n",
      "iteration 300 / 1500: loss 7.543829\n",
      "iteration 400 / 1500: loss 10.103356\n",
      "iteration 500 / 1500: loss 10.806016\n",
      "iteration 600 / 1500: loss 8.961738\n",
      "iteration 700 / 1500: loss 10.111523\n",
      "iteration 800 / 1500: loss 9.694078\n",
      "iteration 900 / 1500: loss 9.117096\n",
      "iteration 1000 / 1500: loss 7.710907\n",
      "iteration 1100 / 1500: loss 9.461746\n",
      "iteration 1200 / 1500: loss 9.642037\n",
      "iteration 1300 / 1500: loss 8.591732\n",
      "iteration 1400 / 1500: loss 10.968015\n",
      "iteration 0 / 1500: loss 781.216990\n",
      "iteration 100 / 1500: loss 6.482148\n",
      "iteration 200 / 1500: loss 5.088954\n",
      "iteration 300 / 1500: loss 7.554132\n",
      "iteration 400 / 1500: loss 7.111759\n",
      "iteration 500 / 1500: loss 6.969873\n",
      "iteration 600 / 1500: loss 6.323467\n",
      "iteration 700 / 1500: loss 8.176678\n",
      "iteration 800 / 1500: loss 5.814445\n",
      "iteration 900 / 1500: loss 6.748768\n",
      "iteration 1000 / 1500: loss 7.614227\n",
      "iteration 1100 / 1500: loss 4.434176\n",
      "iteration 1200 / 1500: loss 6.682210\n",
      "iteration 1300 / 1500: loss 6.345858\n",
      "iteration 1400 / 1500: loss 6.902266\n",
      "iteration 0 / 1500: loss 1541.918674\n",
      "iteration 100 / 1500: loss 12.477431\n",
      "iteration 200 / 1500: loss 10.275302\n",
      "iteration 300 / 1500: loss 10.351246\n",
      "iteration 400 / 1500: loss 10.713818\n",
      "iteration 500 / 1500: loss 9.000323\n",
      "iteration 600 / 1500: loss 11.385365\n",
      "iteration 700 / 1500: loss 12.429349\n",
      "iteration 800 / 1500: loss 11.349918\n",
      "iteration 900 / 1500: loss 10.307933\n",
      "iteration 1000 / 1500: loss 9.719972\n",
      "iteration 1100 / 1500: loss 10.834739\n",
      "iteration 1200 / 1500: loss 10.723985\n",
      "iteration 1300 / 1500: loss 11.070814\n",
      "iteration 1400 / 1500: loss 9.007575\n",
      "iteration 0 / 1500: loss 767.397221\n",
      "iteration 100 / 1500: loss 5.574088\n",
      "iteration 200 / 1500: loss 7.829187\n",
      "iteration 300 / 1500: loss 5.623903\n",
      "iteration 400 / 1500: loss 5.000425\n",
      "iteration 500 / 1500: loss 6.244683\n",
      "iteration 600 / 1500: loss 8.023224\n",
      "iteration 700 / 1500: loss 5.399704\n",
      "iteration 800 / 1500: loss 7.847893\n",
      "iteration 900 / 1500: loss 4.106566\n",
      "iteration 1000 / 1500: loss 6.726171\n",
      "iteration 1100 / 1500: loss 6.777972\n",
      "iteration 1200 / 1500: loss 6.096486\n",
      "iteration 1300 / 1500: loss 6.544746\n",
      "iteration 1400 / 1500: loss 7.049190\n",
      "iteration 0 / 1500: loss 1554.955550\n",
      "iteration 100 / 1500: loss 10.896752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 200 / 1500: loss 12.493882\n",
      "iteration 300 / 1500: loss 11.166934\n",
      "iteration 400 / 1500: loss 10.885315\n",
      "iteration 500 / 1500: loss 12.505615\n",
      "iteration 600 / 1500: loss 10.737546\n",
      "iteration 700 / 1500: loss 12.722705\n",
      "iteration 800 / 1500: loss 13.041186\n",
      "iteration 900 / 1500: loss 12.343258\n",
      "iteration 1000 / 1500: loss 12.447243\n",
      "iteration 1100 / 1500: loss 12.585495\n",
      "iteration 1200 / 1500: loss 12.502766\n",
      "iteration 1300 / 1500: loss 11.409977\n",
      "iteration 1400 / 1500: loss 11.378075\n",
      "iteration 0 / 1500: loss 774.265684\n",
      "iteration 100 / 1500: loss 5.850169\n",
      "iteration 200 / 1500: loss 4.831183\n",
      "iteration 300 / 1500: loss 7.572967\n",
      "iteration 400 / 1500: loss 6.346964\n",
      "iteration 500 / 1500: loss 6.074633\n",
      "iteration 600 / 1500: loss 6.828940\n",
      "iteration 700 / 1500: loss 7.848380\n",
      "iteration 800 / 1500: loss 5.212709\n",
      "iteration 900 / 1500: loss 5.721803\n",
      "iteration 1000 / 1500: loss 6.419801\n",
      "iteration 1100 / 1500: loss 7.794178\n",
      "iteration 1200 / 1500: loss 5.087548\n",
      "iteration 1300 / 1500: loss 6.599682\n",
      "iteration 1400 / 1500: loss 8.172880\n",
      "iteration 0 / 1500: loss 1508.484941\n",
      "iteration 100 / 1500: loss 12.704600\n",
      "iteration 200 / 1500: loss 13.962062\n",
      "iteration 300 / 1500: loss 14.270114\n",
      "iteration 400 / 1500: loss 15.388918\n",
      "iteration 500 / 1500: loss 11.783932\n",
      "iteration 600 / 1500: loss 13.826040\n",
      "iteration 700 / 1500: loss 12.971060\n",
      "iteration 800 / 1500: loss 12.707791\n",
      "iteration 900 / 1500: loss 17.520364\n",
      "iteration 1000 / 1500: loss 13.499767\n",
      "iteration 1100 / 1500: loss 16.584003\n",
      "iteration 1200 / 1500: loss 13.139417\n",
      "iteration 1300 / 1500: loss 14.463001\n",
      "iteration 1400 / 1500: loss 14.215438\n",
      "iteration 0 / 1500: loss 779.394439\n",
      "iteration 100 / 1500: loss 7.054393\n",
      "iteration 200 / 1500: loss 8.804710\n",
      "iteration 300 / 1500: loss 7.745725\n",
      "iteration 400 / 1500: loss 7.186536\n",
      "iteration 500 / 1500: loss 8.790058\n",
      "iteration 600 / 1500: loss 5.985731\n",
      "iteration 700 / 1500: loss 7.039545\n",
      "iteration 800 / 1500: loss 7.690831\n",
      "iteration 900 / 1500: loss 6.899277\n",
      "iteration 1000 / 1500: loss 7.143088\n",
      "iteration 1100 / 1500: loss 5.874551\n",
      "iteration 1200 / 1500: loss 6.389073\n",
      "iteration 1300 / 1500: loss 5.131879\n",
      "iteration 1400 / 1500: loss 9.226944\n",
      "iteration 0 / 1500: loss 1525.459505\n",
      "iteration 100 / 1500: loss 15.604141\n",
      "iteration 200 / 1500: loss 13.963758\n",
      "iteration 300 / 1500: loss 18.266178\n",
      "iteration 400 / 1500: loss 16.797005\n",
      "iteration 500 / 1500: loss 15.767867\n",
      "iteration 600 / 1500: loss 15.511345\n",
      "iteration 700 / 1500: loss 14.122714\n",
      "iteration 800 / 1500: loss 14.563339\n",
      "iteration 900 / 1500: loss 15.232143\n",
      "iteration 1000 / 1500: loss 14.131707\n",
      "iteration 1100 / 1500: loss 17.508171\n",
      "iteration 1200 / 1500: loss 16.974361\n",
      "iteration 1300 / 1500: loss 15.513190\n",
      "iteration 1400 / 1500: loss 16.093032\n",
      "lr 1.000000e-07 reg 2.500000e+04 train accuracy: 0.331816 val accuracy: 0.338000\n",
      "lr 1.000000e-07 reg 5.000000e+04 train accuracy: 0.305000 val accuracy: 0.323000\n",
      "lr 5.000000e-07 reg 2.500000e+04 train accuracy: 0.326143 val accuracy: 0.353000\n",
      "lr 5.000000e-07 reg 5.000000e+04 train accuracy: 0.302939 val accuracy: 0.317000\n",
      "lr 9.000000e-07 reg 2.500000e+04 train accuracy: 0.320878 val accuracy: 0.328000\n",
      "lr 9.000000e-07 reg 5.000000e+04 train accuracy: 0.299816 val accuracy: 0.310000\n",
      "lr 1.300000e-06 reg 2.500000e+04 train accuracy: 0.322776 val accuracy: 0.327000\n",
      "lr 1.300000e-06 reg 5.000000e+04 train accuracy: 0.275510 val accuracy: 0.282000\n",
      "lr 1.700000e-06 reg 2.500000e+04 train accuracy: 0.320776 val accuracy: 0.336000\n",
      "lr 1.700000e-06 reg 5.000000e+04 train accuracy: 0.294837 val accuracy: 0.311000\n",
      "lr 2.100000e-06 reg 2.500000e+04 train accuracy: 0.303449 val accuracy: 0.310000\n",
      "lr 2.100000e-06 reg 5.000000e+04 train accuracy: 0.269735 val accuracy: 0.274000\n",
      "lr 2.500000e-06 reg 2.500000e+04 train accuracy: 0.307898 val accuracy: 0.328000\n",
      "lr 2.500000e-06 reg 5.000000e+04 train accuracy: 0.248857 val accuracy: 0.254000\n",
      "lr 2.900000e-06 reg 2.500000e+04 train accuracy: 0.280265 val accuracy: 0.288000\n",
      "lr 2.900000e-06 reg 5.000000e+04 train accuracy: 0.257857 val accuracy: 0.276000\n",
      "lr 3.300000e-06 reg 2.500000e+04 train accuracy: 0.261041 val accuracy: 0.272000\n",
      "lr 3.300000e-06 reg 5.000000e+04 train accuracy: 0.258020 val accuracy: 0.277000\n",
      "lr 3.700000e-06 reg 2.500000e+04 train accuracy: 0.306347 val accuracy: 0.306000\n",
      "lr 3.700000e-06 reg 5.000000e+04 train accuracy: 0.197571 val accuracy: 0.197000\n",
      "lr 4.100000e-06 reg 2.500000e+04 train accuracy: 0.276490 val accuracy: 0.306000\n",
      "lr 4.100000e-06 reg 5.000000e+04 train accuracy: 0.216020 val accuracy: 0.218000\n",
      "lr 4.500000e-06 reg 2.500000e+04 train accuracy: 0.277755 val accuracy: 0.288000\n",
      "lr 4.500000e-06 reg 5.000000e+04 train accuracy: 0.195714 val accuracy: 0.208000\n",
      "lr 4.900000e-06 reg 2.500000e+04 train accuracy: 0.214531 val accuracy: 0.238000\n",
      "lr 4.900000e-06 reg 5.000000e+04 train accuracy: 0.163694 val accuracy: 0.180000\n",
      "lr 5.300000e-06 reg 2.500000e+04 train accuracy: 0.190286 val accuracy: 0.198000\n",
      "lr 5.300000e-06 reg 5.000000e+04 train accuracy: 0.142571 val accuracy: 0.146000\n",
      "lr 5.700000e-06 reg 2.500000e+04 train accuracy: 0.192469 val accuracy: 0.188000\n",
      "lr 5.700000e-06 reg 5.000000e+04 train accuracy: 0.113245 val accuracy: 0.111000\n",
      "lr 6.100000e-06 reg 2.500000e+04 train accuracy: 0.165184 val accuracy: 0.158000\n",
      "lr 6.100000e-06 reg 5.000000e+04 train accuracy: 0.131000 val accuracy: 0.124000\n",
      "lr 6.500000e-06 reg 2.500000e+04 train accuracy: 0.164837 val accuracy: 0.164000\n",
      "lr 6.500000e-06 reg 5.000000e+04 train accuracy: 0.097735 val accuracy: 0.085000\n",
      "lr 6.900000e-06 reg 2.500000e+04 train accuracy: 0.130898 val accuracy: 0.155000\n",
      "lr 6.900000e-06 reg 5.000000e+04 train accuracy: 0.089816 val accuracy: 0.072000\n",
      "lr 7.300000e-06 reg 2.500000e+04 train accuracy: 0.133469 val accuracy: 0.143000\n",
      "lr 7.300000e-06 reg 5.000000e+04 train accuracy: 0.087102 val accuracy: 0.088000\n",
      "lr 7.700000e-06 reg 2.500000e+04 train accuracy: 0.104959 val accuracy: 0.097000\n",
      "lr 7.700000e-06 reg 5.000000e+04 train accuracy: 0.091265 val accuracy: 0.075000\n",
      "lr 8.100000e-06 reg 2.500000e+04 train accuracy: 0.165980 val accuracy: 0.166000\n",
      "lr 8.100000e-06 reg 5.000000e+04 train accuracy: 0.088939 val accuracy: 0.087000\n",
      "lr 8.500000e-06 reg 2.500000e+04 train accuracy: 0.160082 val accuracy: 0.147000\n",
      "lr 8.500000e-06 reg 5.000000e+04 train accuracy: 0.084531 val accuracy: 0.080000\n",
      "lr 8.900000e-06 reg 2.500000e+04 train accuracy: 0.128898 val accuracy: 0.120000\n",
      "lr 8.900000e-06 reg 5.000000e+04 train accuracy: 0.087980 val accuracy: 0.102000\n",
      "lr 9.300000e-06 reg 2.500000e+04 train accuracy: 0.135041 val accuracy: 0.131000\n",
      "lr 9.300000e-06 reg 5.000000e+04 train accuracy: 0.121816 val accuracy: 0.115000\n",
      "lr 9.700000e-06 reg 2.500000e+04 train accuracy: 0.108327 val accuracy: 0.097000\n",
      "lr 9.700000e-06 reg 5.000000e+04 train accuracy: 0.103816 val accuracy: 0.120000\n",
      "best validation accuracy achieved during cross-validation: 0.353000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "# learning_rates = [1e-7, 5e-7]\n",
    "learning_rate = [i * 1e-7 for i in range(1, 100, 4)]\n",
    "regularization_strengths = [2.5e4, 5e4]\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "for ls in learning_rate:\n",
    "    for rs in regularization_strengths:\n",
    "        softmax = Softmax()\n",
    "        softmax.train(X_train, y_train, learning_rate = ls, reg = rs, num_iters = 1500,verbose = True)\n",
    "        y_train_pred = softmax.predict(X_train)\n",
    "        accu_train = np.mean(y_train_pred == y_train)\n",
    "        y_val_pred = softmax.predict(X_val)\n",
    "        accu_val = np.mean(y_val_pred == y_val)\n",
    "        results[(ls,rs)] = [accu_train, accu_val]\n",
    "        if accu_val > best_val:\n",
    "            best_val = accu_val\n",
    "            best_softmax = softmax\n",
    "pass\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.343000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "**Inline Question 2** - *True or False*\n",
    "\n",
    "Suppose the overall training loss is defined as the sum of the per-datapoint loss over all training examples. It is possible to add a new datapoint to a training set that would leave the SVM loss unchanged, but this is not the case with the Softmax classifier loss.\n",
    "\n",
    "$\\color{blue}{\\textit Your Answer:}$\n",
    "\n",
    "\n",
    "$\\color{blue}{\\textit Your Explanation:}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
